{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DimensionalityReduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4wv68jcux+cK76TTiEWgX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcfatbeard57/Hands-On-ML-Tensor-FLow/blob/main/DimensionalityReduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BEFKGNZDAMd"
      },
      "source": [
        "Chapter 8 : Dimensionality Reduction\r\n",
        "\r\n",
        "Filter out noise and unnecessary details. Also speed up the training\r\n",
        "\r\n",
        "Useful for data viz and finding patterns and clusters\r\n",
        "\r\n",
        "Reduces Space\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eumNcZDbFe26"
      },
      "source": [
        "## Curse of Dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7_OM8L_GLGe"
      },
      "source": [
        "The curse of dimensionality refers to the fact that many problems that do not\r\n",
        "exist in low-dimensional space arise in high-dimensional space. \r\n",
        "\r\n",
        "In Machine Learning, one common manifestation is the fact that randomly sampled high dimensional\r\n",
        "vectors are generally very sparse, increasing the risk of overfitting\r\n",
        "and making it very difficult to identify patterns in the data without having plenty\r\n",
        "of training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CghFlVZGR4n"
      },
      "source": [
        "High dimensional data sets are at risk of being highly sparse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-QudHkDGufi"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evtkQTO9GwAi"
      },
      "source": [
        "First it identifies the hyperplane that lies closest to the data, and then\r\n",
        "it projects the data onto it while preserving the maximum\r\n",
        "variance\r\n",
        "\r\n",
        "it is the axis that minimizes the mean squared distance\r\n",
        "between the original dataset and its projection onto that axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q55u6GiAKxE"
      },
      "source": [
        "from sklearn.decomposition import PCA\r\n",
        "pca = PCA(n_components = 2)\r\n",
        "X2D = pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJokOv8HJEMi"
      },
      "source": [
        "# First principal component is equal to \r\n",
        "pca.components_.T[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAuQcXQJB3p"
      },
      "source": [
        "#### Explained Variance Ratio\r\n",
        "the proportion of the dataset’s variance that lies along the axis of each principal component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2xx4OjYJLtF"
      },
      "source": [
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EPNgi6dJTyg"
      },
      "source": [
        "#### Choosing the Right Number of Dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdhNODhONhBO"
      },
      "source": [
        "choose the number of dimensions that add up to a sufficiently\r\n",
        "large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality\r\n",
        "for data visualization—in that case you will generally want to reduce the\r\n",
        "dimensionality down to 2 or 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hmxmLJ6JTl_"
      },
      "source": [
        "pca = PCA()\r\n",
        "pca.fit(X)\r\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\r\n",
        "d = np.argmax(cumsum >= 0.95) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz7oNP4PNkSV"
      },
      "source": [
        "instead of specifying the number of principal components you want to\r\n",
        "preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the\r\n",
        "ratio of variance you wish to preserve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rS_MR9DNnyC"
      },
      "source": [
        "pca = PCA(n_components=0.95)\r\n",
        "X_reduced = pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaKdO-bPNoNK"
      },
      "source": [
        "**intrinsic dimensionality of the dataset**\r\n",
        "elbow in the\r\n",
        "curve, where the explained variance stops growing fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjiwSoYyN9Ie"
      },
      "source": [
        "#### Reconstruction Error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mNac4fFOFQx"
      },
      "source": [
        "since the projection lost a bit of information (within the\r\n",
        "5% variance that was dropped), The mean squared distance between the original data and the reconstructed data\r\n",
        "(compressed and then decompressed) is called the reconstruction error.\r\n",
        "\r\n",
        "**the inverse_transform() method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOVd-DaTOJ81"
      },
      "source": [
        "pca = PCA(n_components = 154)\r\n",
        "X_mnist_reduced = pca.fit_transform(X_mnist)\r\n",
        "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYaJKJK8OUJ6"
      },
      "source": [
        "## Incremental PCA\r\n",
        "\r\n",
        "split the training\r\n",
        "set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\r\n",
        "useful for large training sets, and also to apply PCA online"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFrIRlCUOsbu"
      },
      "source": [
        "using NumPy’s array_split() function\r\n",
        "\r\n",
        "**partial_fit() method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbuuphjqOJ6f"
      },
      "source": [
        "\r\n",
        "from sklearn.decomposition import IncrementalPCA\r\n",
        "n_batches = 100\r\n",
        "inc_pca = IncrementalPCA(n_components=154)\r\n",
        "for X_batch in np.array_split(X_mnist, n_batches):\r\n",
        "inc_pca.partial_fit(X_batch)\r\n",
        "X_mnist_reduced = inc_pca.transform(X_mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsCm5QryO2mI"
      },
      "source": [
        "NumPy’s memmap class, which allows you to manipulate a\r\n",
        "large array stored in a binary file on disk as if it were entirely in memory; the class\r\n",
        "loads only the data it needs in memory, when it needs it.\r\n",
        "\r\n",
        "usual fit() method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpNXfpIsNurj"
      },
      "source": [
        "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\r\n",
        "batch_size = m // n_batches\r\n",
        "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\r\n",
        "inc_pca.fit(X_mm) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFP91PuRO_Ej"
      },
      "source": [
        "## Randomized PCA\r\n",
        "This\r\n",
        "is a **stochastic algorithm** that quickly finds an approximation of the first d principal\r\n",
        "components. Its computational complexity is O(m × d2) + O(d3), instead of O(m × n2) + O(n3), so it is dramatically **faster** than the previous algorithms  **when d is much smaller than n.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGFMHOsBO_80"
      },
      "source": [
        "\r\n",
        "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\r\n",
        "X_reduced = rnd_pca.fit_transform(X_mnist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mk36aAyPbc9"
      },
      "source": [
        "## Kernel PCA\r\n",
        "\r\n",
        "possible to perform\r\n",
        "complex nonlinear projections for dimensionality reduction.\r\n",
        "\r\n",
        "often **good** at preserving clusters of instances after projection, or\r\n",
        "sometimes even unrolling datasets that lie close to a twisted manifold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8j_VY6EPc0P"
      },
      "source": [
        "from sklearn.decomposition import KernelPCA\r\n",
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\r\n",
        "X_reduced = rbf_pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY2cFaXUPwGt"
      },
      "source": [
        "## Selecting a Kernel and Tuning Hyperparameters\r\n",
        "\r\n",
        "kPCA is an unsupervised learning algorithm, there is no obvious performance\r\n",
        "measure to help you select the best kernel and hyperparameter values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcMc8SFkQB1g"
      },
      "source": [
        "dimensionality reduction is often a preparation step for a supervised learning task\r\n",
        "(e.g., classification), so you can simply use grid search to select the kernel and hyperparameters\r\n",
        "that lead to the best performance on that task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA7RBNH8P2pg"
      },
      "source": [
        "# the following code creates a two-step pipeline, \r\n",
        "# first reducing dimensionality to two dimensions using kPCA, \r\n",
        "# then applying Logistic Regression for classification\r\n",
        "# Then it uses Grid SearchCV to find the best kernel and gamma value for kPCA \r\n",
        "# in order to get the best classification accuracy at the end of the pipeline:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQfp6sSXQRtC"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.pipeline import Pipeline\r\n",
        "clf = Pipeline([\r\n",
        "(\"kpca\", KernelPCA(n_components=2)),\r\n",
        "(\"log_reg\", LogisticRegression())\r\n",
        "])\r\n",
        "param_grid = [{\r\n",
        "\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\r\n",
        "\"kpca__kernel\": [\"rbf\", \"sigmoid\"]\r\n",
        "}]\r\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=3)\r\n",
        "grid_search.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzYnDGTqQWEN"
      },
      "source": [
        "print(grid_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzG1QCKDQ0qO"
      },
      "source": [
        "### Reconstruction Error\r\n",
        "\r\n",
        "Another approach, this time entirely unsupervised, is to select the kernel and hyperparameters\r\n",
        "that yield the lowest reconstruction error.\r\n",
        "\r\n",
        "You can then select the kernel\r\n",
        "and hyperparameters that minimize this reconstruction pre-image error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBdINoDtRN6Q"
      },
      "source": [
        "**set fit_inverse_transform=True**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahoYXZQmQ80k"
      },
      "source": [
        "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\r\n",
        "fit_inverse_transform=True)\r\n",
        "X_reduced = rbf_pca.fit_transform(X)\r\n",
        "X_preimage = rbf_pca.inverse_transform(X_reduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxV82bXpRUlM"
      },
      "source": [
        "KernelPCA has no\r\n",
        "inverse_transform() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvHuAOUXRZgF"
      },
      "source": [
        "compute the reconstruction pre-image error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXILvZDhRaZC"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\r\n",
        "mean_squared_error(X, X_preimage)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptje6MpxRdVk"
      },
      "source": [
        "## LLE\r\n",
        "Locally Linear Embedding is nonlinear dimensionality\r\n",
        "reduction (NLDR) technique.\r\n",
        "\r\n",
        "LLE works by first measuring\r\n",
        "how each training instance linearly relates to its closest neighbors (c.n.), and then\r\n",
        "looking for a low-dimensional representation of the training set where these local\r\n",
        "relationships are best preserved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzvezBo4Rq88"
      },
      "source": [
        "**LocallyLinearEmbedding class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2gB7nK5Rc0h"
      },
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\r\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\r\n",
        "X_reduced = lle.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfoeH_IDRt1I"
      },
      "source": [
        "LLE implementation has the following computational complexity:\r\n",
        "O(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\r\n",
        "weights, and O(dm2) for constructing the low-dimensional representations. Unfortunately,\r\n",
        "the m2 in the last term makes this algorithm scale poorly to very large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnTa9pDQR7ZS"
      },
      "source": [
        "## Other Dimensionality Reduction Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt7dOzkOSATM"
      },
      "source": [
        "*   **Multidimensional Scaling (MDS)** reduces dimensionality while trying to preserve\r\n",
        "the distances between the instances \r\n",
        "\r\n",
        "*   **Isomap** creates a graph by connecting each instance to its nearest neighbors, then\r\n",
        "reduces dimensionality while trying to preserve the geodesic distances9 between\r\n",
        "the instances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft4geLmNSvRI"
      },
      "source": [
        "* **t-Distributed Stochastic Neighbor Embedding (t-SNE)** reduces dimensionality\r\n",
        "while trying to keep similar instances close and dissimilar instances apart. It is\r\n",
        "mostly used for visualization, in particular to visualize clusters of instances in\r\n",
        "high-dimensional space (e.g., to visualize the MNIST images in 2D)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxuvHvMtSyJA"
      },
      "source": [
        "*   **Linear Discriminant Analysis (LDA)** is actually a classification algorithm, but during\r\n",
        "training it learns the most discriminative axes between the classes, and these\r\n",
        "axes can then be used to define a hyperplane onto which to project the data. The\r\n",
        "benefit is that the projection will keep classes as far apart as possible, so LDA is a\r\n",
        "good technique to reduce dimensionality before running another classification\r\n",
        "algorithm such as an SVM classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Esn_pKHCR4jC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}