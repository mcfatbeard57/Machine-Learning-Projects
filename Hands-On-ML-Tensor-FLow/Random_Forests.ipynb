{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Random_Forests.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Y9naYVy2qzSz",
        "HQLJSJfbtH0-",
        "p23Iex1HxZUk",
        "jYVz21FW6u2s",
        "QYXSJNjxKkAg",
        "FQN4m9vuKmr1",
        "WIXtJn7SQXLO"
      ],
      "authorship_tag": "ABX9TyNw6KyFwbzUBXEF4KnFcTOf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcfatbeard57/Hands-On-ML-Tensor-FLow/blob/main/Random_Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MSJLsUE4BRc"
      },
      "source": [
        "Chapter 7 : Ensemble Learning and Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9naYVy2qzSz"
      },
      "source": [
        "## Ensemble\r\n",
        "Trades more bias for lower vairance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpbUVutBQwtk"
      },
      "source": [
        "Ensemble methods work best when the predictors are as independent\r\n",
        "from one another as possible. One way to get diverse classifiers\r\n",
        "is to train them using very different algorithms. This increases the\r\n",
        "chance that they will make very different types of errors, improving\r\n",
        "the ensemble’s accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTHznkRTq33O"
      },
      "source": [
        "### Hard Voting\r\n",
        "Majority-vote classifier is a an classifier which aggregate the predictions of\r\n",
        "each classifier and predict the class that gets the most votes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWPt9DE13_yz"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn.ensemble import VotingClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import SVC\r\n",
        "\r\n",
        "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\r\n",
        "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\r\n",
        "svm_clf = SVC(gamma=\"auto\", random_state=42)\r\n",
        "\r\n",
        "voting_clf = VotingClassifier(\r\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\r\n",
        "    voting='hard')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90EYJZQWRH8W"
      },
      "source": [
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lhYfJg9RKdb"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\r\n",
        "    clf.fit(X_train, y_train)\r\n",
        "    y_pred = clf.predict(X_test)\r\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlTgZf-squUk"
      },
      "source": [
        "### Soft voting.\r\n",
        "If all classifiers are able to estimate class probabilities (i.e., they have a **predict_proba() method**), then you can tell Scikit-Learn to predict the class with the\r\n",
        "highest class probability, averaged over all the individual classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRgcwm2BrbAH"
      },
      "source": [
        "Achieves higher performance than hard voting because it gives more\r\n",
        "weight to highly confident votes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF1IvmS3rmxP"
      },
      "source": [
        "\r\n",
        "replace voting=\"hard\" with **voting=\"soft\"** and ensure that all classifiers can estimate class probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izfpcw38uGtZ"
      },
      "source": [
        "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\r\n",
        "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\r\n",
        "#svm with predict_proba() method\r\n",
        "svm_clf = SVC(gamma=\"auto\", probability=True, random_state=42)\r\n",
        "\r\n",
        "voting_clf = VotingClassifier(\r\n",
        "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\r\n",
        "    voting='soft')\r\n",
        "voting_clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\r\n",
        "    clf.fit(X_train, y_train)\r\n",
        "    y_pred = clf.predict(X_test)\r\n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g69tNK43DnN0"
      },
      "source": [
        "**Hard Vs Soft**\r\n",
        "\r\n",
        "Hard voting counts votes of each classifer in ensemble and picks that gets most votes.\r\n",
        "\r\n",
        "While soft voting computes avg. estimated class probability for each class and picks the class with highest  probability. This gives confidence votes more weights and perform better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQLJSJfbtH0-"
      },
      "source": [
        "## Bagging and Pasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcgSYIimtReQ"
      },
      "source": [
        "use the same training algorithm for every\r\n",
        "predictor, but to train them on different random subsets of the training set. When\r\n",
        "sampling is performed with replacement, this method is called bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-tX7juPtctC"
      },
      "source": [
        "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors the most frequent prediction for classification, or the average for regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5sAlESoto5m"
      },
      "source": [
        "Each individual\r\n",
        "predictor has a higher bias than if it were trained on the original training set, but\r\n",
        "aggregation reduces both bias and variance.\r\n",
        "\r\n",
        "ensemble has a **similar bias but a lower variance** than a single predictor trained on the\r\n",
        "original training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLe-PiTqrkrm"
      },
      "source": [
        "\r\n",
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "\r\n",
        "bag_clf = BaggingClassifier(\r\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\r\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\r\n",
        "bag_clf.fit(X_train, y_train)\r\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjWxb0nXxZij"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wq-_GX7x8sS"
      },
      "source": [
        "Automatically performs soft voting if base classifier has can estimate class probabilities i.e. predict_proba() method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJY9Pc2wrxNL"
      },
      "source": [
        "#### OOB score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywH5TE_ksAlv"
      },
      "source": [
        "By default a BaggingClassifier samples m\r\n",
        "training instances with **replacement (bootstrap=True)**,\r\n",
        "This means that only about 63% of the training instances are sampled on\r\n",
        "average for each predictor. The remaining 37% of the training instances that are not\r\n",
        "sampled are called out-of-bag (oob) instances. Note that they are not the same 37%\r\n",
        "for all predictors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpBN4L9EDbtA"
      },
      "source": [
        "This gives extar instances to be trained on without having additional validation set and ensemble performs slightly berrer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiXiF5qmsLMz"
      },
      "source": [
        "**set oob_score=True**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz_7Srh2seC0"
      },
      "source": [
        "bag_clf = BaggingClassifier(\r\n",
        "              DecisionTreeClassifier(), n_estimators=500,\r\n",
        "              bootstrap=True, n_jobs=-1, oob_score=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8Hg2fIYslR2"
      },
      "source": [
        "bag_clf.fit(X_train, y_train)\r\n",
        "bag_clf.oob_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTIMdcEZsraH"
      },
      "source": [
        "The oob decision function for each training instance is also available through the\r\n",
        "**oob_decision_function_** variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8pUaCR45qSX"
      },
      "source": [
        "bag_clf.oob_decision_function_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FpfXlIws6Pi"
      },
      "source": [
        "two hyperparameters: **max_features and bootstrap_features**. They work\r\n",
        "the same way as **max_samples and bootstrap**, but for feature sampling instead of\r\n",
        "instance sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiJLmnCqtLU4"
      },
      "source": [
        "useful when you are dealing with high-dimensional inputs (such\r\n",
        "as images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o3mlgTXtVF2"
      },
      "source": [
        "**NOTE** :\r\n",
        "\r\n",
        "Sampling both training instances and features is called the   ***Random\r\n",
        "Patches method***.\r\n",
        "\r\n",
        "Keeping all training instances (i.e., bootstrap=False and max_sam\r\n",
        "ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_features smaller than 1.0) is called the ***Random Subspaces method***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p23Iex1HxZUk"
      },
      "source": [
        "## Random Forests\r\n",
        "ensemble of Decision Trees, trained via the bagging method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQTODLBJ2CpX"
      },
      "source": [
        "# Using BaggingClassifier\r\n",
        "bag_clf = BaggingClassifier(\r\n",
        "          DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\r\n",
        "          n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)\r\n",
        "\r\n",
        "bag_clf.fit(X_train, y_train)\r\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ExxUlG3YRP"
      },
      "source": [
        "There are a few notable exceptions: \r\n",
        "\r\n",
        "splitter is absent (forced to \"random\"), \r\n",
        "\r\n",
        "presort is absent (forced to False), \r\n",
        "\r\n",
        "max_samples is absent (forced to 1.0), \r\n",
        "\r\n",
        "and base_estimator is absent (forced to DecisionTreeClassifier with the provided hyperparameters)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBy--x3D2S0f"
      },
      "source": [
        "# Using RandomForestClassifier\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "\r\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\r\n",
        "rnd_clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--JmOn944iIr"
      },
      "source": [
        "#### Extremely Randomized Trees ensemble (or Extra-Trees for short).\r\n",
        "\r\n",
        "They uses random threshhold for each feature instaed of finding best threshold like decision trees does. This acts as a form of regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyvcsF1k4mOm"
      },
      "source": [
        "# ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. \r\n",
        "# Similarly, the ExtraTreesRegressor class has the same API as the RandomForestRegressor class."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7fYwdI04cEc"
      },
      "source": [
        "#### Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqfMtvS4zTx"
      },
      "source": [
        "important features are likely to appear\r\n",
        "closer to the root of the tree, while unimportant features will often appear closer to\r\n",
        "the leaves (or not at all)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D885siQm42S-"
      },
      "source": [
        "**feature_importances_ variable.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQudNgpF2ZBc"
      },
      "source": [
        "from sklearn.datasets import load_iris\r\n",
        "iris = load_iris()\r\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\r\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\r\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\r\n",
        "    print(name, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0568ZDMt47Jl"
      },
      "source": [
        "rnd_clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdWU8ooA5LgT"
      },
      "source": [
        "**Random Forests are very handy to get a quick understanding of what features\r\n",
        "actually matter, in particular if you need to perform feature selection.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYVz21FW6u2s"
      },
      "source": [
        "## Boosting\r\n",
        "train predictors sequentially, each trying to correct its predecessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWsPHeOs6yZj"
      },
      "source": [
        "### AdaBoost\r\n",
        "pay a bit more attention\r\n",
        "to the training instances that the predecessor underfitted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2M0nSxZBB9L"
      },
      "source": [
        "a first base classifier (such as a Decision\r\n",
        "Tree) is trained and used to make predictions on the training set. The relative weight\r\n",
        "of misclassified training instances is then increased. A second classifier is trained\r\n",
        "using the updated weights and again it makes predictions on the training set, weights\r\n",
        "are updated, and so on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7s7DV0JA1Vp"
      },
      "source": [
        "this sequential learning technique has some\r\n",
        "similarities with Gradient Descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\r\n",
        "gradually making it better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AZlXAck5NLe"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\r\n",
        "\r\n",
        "ada_clf = AdaBoostClassifier(\r\n",
        "            DecisionTreeClassifier(max_depth=1), n_estimators=200,\r\n",
        "            algorithm=\"SAMME.R\", learning_rate=0.5, random_state=42)\r\n",
        "ada_clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# SAMME is Stagewise Additive Modeling using a Multiclass Exponential loss function\r\n",
        "# When there are just two classes, SAMME is equivalent to AdaBoost.\r\n",
        "# if the predictors can estimate class probabilities Scikit-Learn can use a variant of SAMME called SAMME.R \r\n",
        "# (the R stands for “Real”), which relies on class probabilities rather than predictions and \r\n",
        "# generally performs better."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO_BOC-ICCHu"
      },
      "source": [
        "# A Decision Stump is a Decision Tree with max_depth=1—in\r\n",
        "# other words, a tree composed of a single decision node plus two leaf nodes."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Q8b4eqBMVf"
      },
      "source": [
        "**NOTE:**\r\n",
        "\r\n",
        "SVM are not good base predictor as they are slow and tend to be unstable with AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtU3b4K1BcJh"
      },
      "source": [
        "**Drawback** is that it cannot be parallelized. As a result, does not scale well as bagging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEI0jOTjCGB4"
      },
      "source": [
        "If your AdaBoost ensemble is **overfitting** the training set, you can\r\n",
        "try **reducing the number of estimators** or more  **strongly regularizing**\r\n",
        "the base estimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZdbQa73FIWk"
      },
      "source": [
        "If your AdaBoost ensemble is **underfitting** the training set, you can try **incresing the number of estimators** or **reducing the regularizing** of the base estimator. Also try to silghtly **increase the learning rate.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XoI5_lMCTfo"
      },
      "source": [
        "### Gradient Boosting\r\n",
        "instead of tweaking the instance weights at every\r\n",
        "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\r\n",
        "errors made by the previous predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaUJ_YbSA04m"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\r\n",
        "\r\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)\r\n",
        "gbrt.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLqxQ3XVGlcc"
      },
      "source": [
        "#### Shrinkage\r\n",
        "**Learinng Rate** hyperparameter scales the contribution of each tree.\r\n",
        "\r\n",
        "If it is set to low value, 0.1, then you will need more trees in the ensemble to fit trainng set but predictions will generalize better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTtC6p69Hbfi"
      },
      "source": [
        "gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)\r\n",
        "gbrt_slow.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BlNVWiWLMc8"
      },
      "source": [
        "If it does not have enough trees to fit the training set it **underfits**,\r\n",
        "while it has too many trees and **overfits** the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB1CHfceLh2S"
      },
      "source": [
        "###### NOTE\r\n",
        "IF GB ensemble **overfit** the trainng set then try **decreasing the learning rate**\r\n",
        "Also use **early stopping** to find right no. of predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Yy8eatHYSE"
      },
      "source": [
        "#### Early Stopping\r\n",
        "find the optimal number of trees,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL6PMCU1IUb8"
      },
      "source": [
        "**staged_predict() method**\r\n",
        "it\r\n",
        "returns an iterator over the predictions made by the ensemble at each stage of training\r\n",
        "(with one tree, two trees, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_hkrC-xIz0S"
      },
      "source": [
        "code trains a GBRT ensemble with\r\n",
        "120 trees, then measures the validation error at each stage of training to find the optimal\r\n",
        "number of trees, and finally trains another GBRT ensemble using the optimal\r\n",
        "number of trees:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMkhKQ2GHIlP"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)\r\n",
        "\r\n",
        "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)\r\n",
        "gbrt.fit(X_train, y_train)\r\n",
        "\r\n",
        "errors = [mean_squared_error(y_val, y_pred)\r\n",
        "          for y_pred in gbrt.staged_predict(X_val)]\r\n",
        "bst_n_estimators = np.argmin(errors) + 1\r\n",
        "\r\n",
        "gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)\r\n",
        "gbrt_best.fit(X_train, y_train)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iilJygIkI07v"
      },
      "source": [
        "implement early stopping by actually stopping training early\r\n",
        "(instead of training a large number of trees first and then looking back to find the\r\n",
        "optimal number)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s27UY7ZII6Ic"
      },
      "source": [
        "**warm_start=True**\r\n",
        "\r\n",
        "\r\n",
        "following code stops training when the validation error does not\r\n",
        "improve for five iterations in a row:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nub4fR-aI42s"
      },
      "source": [
        "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True, random_state=42)\r\n",
        "\r\n",
        "min_val_error = float(\"inf\")\r\n",
        "error_going_up = 0\r\n",
        "for n_estimators in range(1, 120):\r\n",
        "    gbrt.n_estimators = n_estimators\r\n",
        "    gbrt.fit(X_train, y_train)\r\n",
        "    y_pred = gbrt.predict(X_val)\r\n",
        "    val_error = mean_squared_error(y_val, y_pred)\r\n",
        "    if val_error < min_val_error:\r\n",
        "        min_val_error = val_error\r\n",
        "        error_going_up = 0\r\n",
        "    else:\r\n",
        "        error_going_up += 1\r\n",
        "        if error_going_up == 5:\r\n",
        "            break  # early stopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlPDXmeCJ_Pw"
      },
      "source": [
        "print(gbrt.n_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Oc0OCBKApC"
      },
      "source": [
        "print(\"Minimum validation MSE:\", min_val_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb4c3_LBJIJ0"
      },
      "source": [
        "#### Stochastic Gradient Boosting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heMDiiDnJYdD"
      },
      "source": [
        "**subsample hyperparameter**\r\n",
        "\r\n",
        "specifies the fraction of training instances to be used for training each tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwQ1KKLUJfg0"
      },
      "source": [
        "this trades a **higher bias for a lower variance**. \r\n",
        "It also **speeds** up training considerably."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f_yi5fWJWod"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdr3vI_RJty8"
      },
      "source": [
        "**loss hyperparameter** helps to choose other cost functions to be used with Gradient Boosting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP819N8WJ7Fz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ezLol5KFgz"
      },
      "source": [
        "### Using XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8sFPcz2KNuz"
      },
      "source": [
        "import xgboost\r\n",
        "xgb_reg = xgboost.XGBRegressor(random_state=42)\r\n",
        "xgb_reg.fit(X_train, y_train)\r\n",
        "y_pred = xgb_reg.predict(X_val)\r\n",
        "val_error = mean_squared_error(y_val, y_pred)\r\n",
        "print(\"Validation MSE:\", val_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JBw62ofKSp1"
      },
      "source": [
        "xgb_reg.fit(X_train, y_train,\r\n",
        "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\r\n",
        "y_pred = xgb_reg.predict(X_val)\r\n",
        "val_error = mean_squared_error(y_val, y_pred)\r\n",
        "print(\"Validation MSE:\", val_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMPek5fxKgVc"
      },
      "source": [
        "%timeit xgboost.XGBRegressor().fit(X_train, y_train) if xgboost is not None else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nklMmk-nKhdA"
      },
      "source": [
        "%timeit GradientBoostingRegressor().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYXSJNjxKkAg"
      },
      "source": [
        "## Excercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQN4m9vuKmr1"
      },
      "source": [
        "### 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm95CwGVO61S"
      },
      "source": [
        "Splitting the dataset into Train, Validation and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRwRL57dKloi"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\r\n",
        "                                                    mnist.data, mnist.target, test_size=10000, random_state=42)\r\n",
        "X_train, X_val, y_train, y_val = train_test_split(\r\n",
        "                                                X_train_val, y_train_val, test_size=10000, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8RsRhckPBMO"
      },
      "source": [
        "train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoXsnaDfPDru"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-gKwM66PFMO"
      },
      "source": [
        "random_forest_clf = RandomForestClassifier(n_estimators=10, random_state=42)\r\n",
        "extra_trees_clf = ExtraTreesClassifier(n_estimators=10, random_state=42)\r\n",
        "svm_clf = LinearSVC(random_state=42)\r\n",
        "mlp_clf = MLPClassifier(random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HBzbGGvPMdo"
      },
      "source": [
        "estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\r\n",
        "for estimator in estimators:\r\n",
        "    print(\"Training the\", estimator)\r\n",
        "    estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfqAlCouPQPE"
      },
      "source": [
        "[estimator.score(X_val, y_val) for estimator in estimators]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajmQIevlPjYa"
      },
      "source": [
        "The linear SVM is far outperformed by the other classifiers. However, let's keep it for now since it may improve the voting classifier's performance.\r\n",
        "\r\n",
        "Exercise: Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndn1QHKVPbTQ"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkO3uPPFPdcQ"
      },
      "source": [
        "named_estimators = [\r\n",
        "    (\"random_forest_clf\", random_forest_clf),\r\n",
        "    (\"extra_trees_clf\", extra_trees_clf),\r\n",
        "    (\"svm_clf\", svm_clf),\r\n",
        "    (\"mlp_clf\", mlp_clf),\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd5OC31MPez-"
      },
      "source": [
        "voting_clf = VotingClassifier(named_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ckIzRMMPm3x"
      },
      "source": [
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDsNhOJ2PqMF"
      },
      "source": [
        "voting_clf.score(X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D28GG2XpPruF"
      },
      "source": [
        "[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe8ghB0EPzXF"
      },
      "source": [
        "Let's remove the SVM to see if performance improves. It is possible to remove an estimator by setting it to None using set_params() like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvQ_6bIkPtBk"
      },
      "source": [
        "voting_clf.set_params(svm_clf=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IanHSYN1P1iO"
      },
      "source": [
        "This updated the list of estimators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnTK-5gVPugo"
      },
      "source": [
        "voting_clf.estimators"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3To7MxFUP36_"
      },
      "source": [
        "\r\n",
        "However, it did not update the list of trained estimators:\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHvC5gNJP47k"
      },
      "source": [
        "voting_clf.estimators_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5Gps711QAMH"
      },
      "source": [
        "So we can either fit the VotingClassifier again, or just remove the SVM from the list of trained estimators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_pvUZdXQAvv"
      },
      "source": [
        "del voting_clf.estimators_[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYYyeddKQC8O"
      },
      "source": [
        "Now let's evaluate the VotingClassifier again:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwoaihvwQDSe"
      },
      "source": [
        "voting_clf.score(X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwNTsvNeQFyR"
      },
      "source": [
        "A bit better! The SVM was hurting performance. Now let's try using a soft voting classifier. We do not actually need to retrain the classifier, we can just set voting to \"soft\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXCJa95lQGT7"
      },
      "source": [
        "voting_clf.voting = \"soft\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siOvOmyuQJHg"
      },
      "source": [
        "voting_clf.score(X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPqCBEN7QK5c"
      },
      "source": [
        "That's a significant improvement, and it's much better than each of the individual classifiers.\r\n",
        "\r\n",
        "Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ane_Z7h6QLZv"
      },
      "source": [
        "voting_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVXJvIbzQM9P"
      },
      "source": [
        "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bABozvrSQPbZ"
      },
      "source": [
        "The voting classifier reduced the error rate from about 4.0% for our best model (the MLPClassifier) to just 3.1%. That's about 22.5% less errors, not bad!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIXtJn7SQXLO"
      },
      "source": [
        "### Stacking Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGXJVP6BQcJQ"
      },
      "source": [
        "Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image's class. Train a classifier on this new training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6XiHHpwQP1g"
      },
      "source": [
        "X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)\r\n",
        "\r\n",
        "for index, estimator in enumerate(estimators):\r\n",
        "    X_val_predictions[:, index] = estimator.predict(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "844YuVn0Qf9i"
      },
      "source": [
        "X_val_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0xWE_RAQh5i"
      },
      "source": [
        "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\r\n",
        "rnd_forest_blender.fit(X_val_predictions, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0LPlW_9QjXg"
      },
      "source": [
        "rnd_forest_blender.oob_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0XqDVARQeqr"
      },
      "source": [
        "You could fine-tune this blender or try other types of blenders (e.g., an MLPClassifier), then select the best one using cross-validation, as always."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUVKECgjQlvN"
      },
      "source": [
        "\r\n",
        "Exercise: Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let's evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble's predictions. How does it compare to the voting classifier you trained earlier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI2VNFkKQod7"
      },
      "source": [
        "X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)\r\n",
        "\r\n",
        "for index, estimator in enumerate(estimators):\r\n",
        "    X_test_predictions[:, index] = estimator.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoXfhQjQQp0X"
      },
      "source": [
        "y_pred = rnd_forest_blender.predict(X_test_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrkmnAbVQq4M"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9JyzOawQsMV"
      },
      "source": [
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOWCT6XmQt6o"
      },
      "source": [
        "This stacking ensemble does not perform as well as the soft voting classifier we trained earlier, it's just as good as the best individual classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDe9CjbkQumy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}