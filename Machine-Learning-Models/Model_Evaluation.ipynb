{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Dummy Classifiers\n",
    "\n",
    "Confusion Matrices and Evaluation Metrices\n",
    "\n",
    "ROC curves, AUC (Area under curve)\n",
    "\n",
    "Evaluation measures for multi-class classification\n",
    "\n",
    "Model Selection/Optimization: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifiers\n",
    "DummyClassifier is a classifier that makes predictions using simple rules, which can be useful as a baseline for comparison against actual classifiers, especially with imbalanced classes.\n",
    "\n",
    "- Provide Null Metrics\n",
    "\n",
    "**It is used as F1-Scoring method, when positive class is in miniority**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies:\n",
    "\n",
    "        most_frequent\n",
    "        statified\n",
    "        uniform\n",
    "        constant\n",
    "Strategies Parameter Option:\n",
    "\n",
    "        mean\n",
    "        median\n",
    "        quantile\n",
    "        constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# The dummy 'most_frequent' classifier always predicts majority (most frequent) class.\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "y_dummy_predictions\n",
    "\n",
    "dummy_majority.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starfied Strategy**\n",
    "\n",
    "Produces random predictions w/ same class proportion as training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_classprop = DummyClassifier(strategy='stratified').fit(X_train, y_train)\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if my classifier accuracy is close to the null accuracy baseline\n",
    "- Ineffective, erroneous or missing features\n",
    "- Poor choice of kernel or hyperparameter\n",
    "- Large class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary (two-class) confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics for binary classification\n",
    "\n",
    "- Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "\n",
    "- Precision = TP / (TP + FP)\n",
    "\n",
    "- Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "\n",
    "- F1 = 2 * Precision * Recall / (Precision + Recall) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_predicted)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, y_predicted)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, y_predicted)))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, y_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combined report with all above metrics**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predicted, target_names=['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoff between precision and recall\n",
    "•Recall-oriented machine learning tasks:\n",
    "\n",
    "        – Search and information extraction in legal discovery\n",
    "        – Tumor detection\n",
    "        – Often paired with a human expert to filter out false positives\n",
    "•Precision-oriented machine learning tasks:\n",
    "\n",
    "        – Search engine ranking, query suggestion\n",
    "        – Document classification\n",
    "        – Many customer-facing tasks (users remember failures!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Function and Predicted Probability\n",
    "\n",
    "I don't understand this yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n",
    "\n",
    "# show the decision_function scores for first 20 instances\n",
    "y_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "y_proba_lr = lr.fit(X_train, y_train).predict_proba(X_test)\n",
    "y_proba_list = list(zip(y_test[0:20], y_proba_lr[0:20,1]))\n",
    "\n",
    "# show the probability of positive class for first 20 instances\n",
    "y_proba_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curves\n",
    "“Steepness” of P-R curves is important:\n",
    "- Top right corner is The “ideal” point where Precision = 1.0 and Recall = 1.0\n",
    "- Maximize precision while maximizing recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([0.0, 1.01])\n",
    "plt.ylim([0.0, 1.01])\n",
    "plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\n",
    "plt.xlabel('Precision', fontsize=16)\n",
    "plt.ylabel('Recall', fontsize=16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves, Area-Under-Curve (AUC)\n",
    "“Steepness” of ROC curves is important:\n",
    "- Top left corner is The “ideal” point where False positive rate of zero True positive rate of one\n",
    "- Maximize the true positive rate while minimizing the false positive rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n",
    "\n",
    "y_score_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_score_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n",
    "plt.xlabel('False Positive Rate', fontsize=16)\n",
    "plt.ylabel('True Positive Rate', fontsize=16)\n",
    "plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "plt.legend(loc='lower right', fontsize=13)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC = 0 (worst) AUC = 1 (best)\n",
    "AUC can be interpreted as:\n",
    "1. The total area under the ROC curve.\n",
    "2. The probability that the classifier will assign a higher score to a randomly chosen positive example than to a randomly chosen negative example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Gives a single number for easy comparison.\n",
    "- Does not require specifying a decision threshold.\n",
    "\n",
    "Drawbacks:\n",
    "- As with other single-number metrics, AUC loses information, e.g. about tradeoffs and the shape of the ROC curve.\n",
    "- This may be a factor to consider when e.g. wanting to compare the performance of classifiers with overlapping ROC curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation measures for multi-class classification\n",
    "\n",
    "A collection of true vs predicted binary outcomes, one per class.\n",
    "\n",
    "**Each instance can have multiple labels**\n",
    "\n",
    "The support (number of instances) for each class is important to consider, e.g. in case of imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and Train Test Split\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Model Creation and predictions\n",
    "svm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "\n",
    "# Confusion Matrix\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_cm = pd.DataFrame(confusion_mc, \n",
    "                     index = [i for i in range(0,10)], columns = [i for i in range(0,10)])\n",
    "\n",
    "# Plotting a heatmap of Confusion Matrix\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('SVM Linear Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n",
    "                                                                       svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-class Evaluation Metrics via the \"Average\" Parameter for a Scoring Function**\n",
    "- Micro: Metric on aggregated instances\n",
    "- Macro: Mean per-class metric, classes have equal weight\n",
    "- Weighted: Mean per-class metric, weighted by support\n",
    "- Samples: for multi-label problems only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-class classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_mc, svm_predicted_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Micro- vs. macro-averaged metrics\n",
    "\n",
    "If some classes are much larger (more instances) than others, and you want to:\n",
    "- Weight your metric toward the largest ones, use micro-averaging.\n",
    "- Weight your metric toward the smallest ones, use macro-averaging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Macro-average** : Each class has equal weight\n",
    "1. Compute metric within each class\n",
    "2. Average resulting metrics across classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Macro-averaged precision = {:.2f} (treat classes equally)'\n",
    "      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))\n",
    "print('Macro-averaged f1 = {:.2f} (treat classes equally)'\n",
    "      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Micro-average**: Each instance has equal weight. Largest classes have most influence.\n",
    "1. Compute metric with aggregate outcomes\n",
    "2. Aggregrate outcomes across all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Micro-averaged precision = {:.2f} (treat instances equally)'\n",
    "      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n",
    "      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the micro-averageis much lower than the macro-average then examine the larger classes for poor metric performance. \n",
    "- If the macro-average is much lower than the micro-average then examine the smaller classes for poor metric performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression evaluation metrics\n",
    "\n",
    "- r2 Score: computes how well future instances will be predicted. Best Score:1.0\n",
    "\n",
    "Alternative metrics include:\n",
    "- mean_absolute_error(absolute difference of target & predicted values)\n",
    "- mean_squared_error(squared difference of target & predicted values)\n",
    "- median_absolute_error(robust to outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean squared error (dummy): {:.2f}\".format(mean_squared_error(y_test, \n",
    "                                                                     y_predict_dummy_mean)))\n",
    "print(\"Mean squared error (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\n",
    "\n",
    "print(\"r2_score (dummy): {:.2f}\".format(r2_score(y_test, y_predict_dummy_mean)))\n",
    "print(\"r2_score (linear model): {:.2f}\".format(r2_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection using evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test on same data\n",
    "- Single metric.\n",
    "- Typically overfits and likely won't generalize well to new data.\n",
    "- But can serve as a sanity check: low accuracy on the training set may indicate an implementation problem.\n",
    "\n",
    "Single train/test split\n",
    "- Single metric.\n",
    "- Speed and simplicity.\n",
    "- Lack of variance information\n",
    "\n",
    "K-fold cross-validation\n",
    "- K train-test splits.\n",
    "- Average metric over all splits.\n",
    "- Can be combined with parameter grid search: GridSearchCV(def. cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "eavl_metric = ('precision','recall', 'f1','roc_auc')\n",
    "\n",
    "# After train test split\n",
    "clf = SVC(kernel='rbf')\n",
    "#Grid Values\n",
    "grid_values = {'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100]}\n",
    "grid_values = {'class_weight':['balanced', {1:2},{1:3},{1:4},{1:5},{1:10},{1:20},{1:50}]}\n",
    "\n",
    "# default metric to optimize over grid parameters: accuracy\n",
    "grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring ='accuracy')\n",
    "\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "print('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_clf_acc.best_score_)\n",
    "\n",
    "y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) \n",
    "print('Test set AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics supported for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "\n",
    "print(sorted(list(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation, and Test Frameworkfor Model Selection and Evaluation\n",
    "\n",
    "- Using only cross-validation or a test set to do model selection may lead to more subtle overfitting / optimistic generalization estimates\n",
    "\n",
    "- Instead, use three data splits:\n",
    "    1. Training set (model building)\n",
    "    2. Validation set (model selection)\n",
    "    3. Test set (final evaluation)\n",
    "\n",
    "In practice:\n",
    "- Create an initial training/test split\n",
    "- Do cross-validation on the training data for model/parameter selection\n",
    "- Save the held-out test set for final model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding Notes\n",
    "\n",
    "- Accuracy is often not the right evaluation metric for many real-world machine learning tasks\n",
    "- False positives and false negatives may need to be treated very differently\n",
    "- Make sure you understand the needs of your application and choose an evaluation metric that matches your application, user, or business goals.\n",
    "\n",
    "Examples of additional evaluation methods include:\n",
    "- Learning curve: How much does accuracy (or other metric) change as a function of the amount of training data?\n",
    "- Sensitivity analysis: How much does accuracy (or other metric) change as a function of key learning parameter values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
