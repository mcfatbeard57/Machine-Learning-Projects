{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyper_Parameter_Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/qQs7L8VZVGiCfByCRFIo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcfatbeard57/Machine-Learning-Models/blob/master/Hyper_Parameter_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P61-aME16Oo5"
      },
      "source": [
        " Hyper Parameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxGNKPLB6UyM"
      },
      "source": [
        "**All Techniques Of Hyper Parameter Optimization**\r\n",
        "\r\n",
        "GridSearchCV\r\n",
        "\r\n",
        "RandomizedSearchCV\r\n",
        "\r\n",
        "Bayesian Optimization -Automate Hyperparameter Tuning (Hyperopt)\r\n",
        "\r\n",
        "Sequential Model Based Optimization(Tuning a scikit-learn estimator with skopt)\r\n",
        "\r\n",
        "Optuna- Automate Hyperparameter Tuning\r\n",
        "\r\n",
        "Genetic Algorithms (TPOT Classifier)\r\n",
        "\r\n",
        "*References*\r\n",
        "\r\n",
        "https://github.com/fmfn/BayesianOptimization\r\n",
        "\r\n",
        "https://github.com/hyperopt/hyperopt\r\n",
        "\r\n",
        "https://www.jeremyjordan.me/hyperparameter-tuning/\r\n",
        "\r\n",
        "https://optuna.org/\r\n",
        "\r\n",
        "https://towardsdatascience.com/hyperparameters-optimization-526348bb8e2d (By Pier Paolo Ippolito )\r\n",
        "\r\n",
        "https://scikit-optimize.github.io/stable/auto_examples/hyperparameter-optimization.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaShisc2505J"
      },
      "source": [
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ER9i387Sgp"
      },
      "source": [
        "## Randomized Search Cv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "430wRuN-7JrJ"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "# Number of trees in random forest\r\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\r\n",
        "# Number of features to consider at every split\r\n",
        "max_features = ['auto', 'sqrt','log2']\r\n",
        "# Maximum number of levels in tree\r\n",
        "max_depth = [int(x) for x in np.linspace(10, 1000,10)]\r\n",
        "# Minimum number of samples required to split a node\r\n",
        "min_samples_split = [2, 5, 10,14]\r\n",
        "# Minimum number of samples required at each leaf node\r\n",
        "min_samples_leaf = [1, 2, 4,6,8]\r\n",
        "# Create the random grid\r\n",
        "random_grid = {'n_estimators': n_estimators,\r\n",
        "               'max_features': max_features,\r\n",
        "               'max_depth': max_depth,\r\n",
        "               'min_samples_split': min_samples_split,\r\n",
        "               'min_samples_leaf': min_samples_leaf,\r\n",
        "              'criterion':['entropy','gini']}\r\n",
        "print(random_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFBbq0Wg7snK"
      },
      "source": [
        "rf=RandomForestClassifier()\r\n",
        "rf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\r\n",
        "                               random_state=100,n_jobs=-1)\r\n",
        "### fit the randomized model\r\n",
        "rf_randomcv.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrfbLM583lb"
      },
      "source": [
        "rf_randomcv.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcXQ9Vxy9Bbb"
      },
      "source": [
        "rf_randomcv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Sxw1qu9HRb"
      },
      "source": [
        "best_random_grid=rf_randomcv.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f05kaV99Kof"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "y_pred=best_random_grid.predict(X_test)\r\n",
        "print(confusion_matrix(y_test,y_pred))\r\n",
        "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\r\n",
        "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH4PZst19W2g"
      },
      "source": [
        "## GridSearch CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3iWTgOb9Xhh"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\r\n",
        "\r\n",
        "param_grid = {\r\n",
        "    'criterion': [rf_randomcv.best_params_['criterion']],\r\n",
        "    'max_depth': [rf_randomcv.best_params_['max_depth']],\r\n",
        "    'max_features': [rf_randomcv.best_params_['max_features']],\r\n",
        "    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], \r\n",
        "                         rf_randomcv.best_params_['min_samples_leaf']+2, \r\n",
        "                         rf_randomcv.best_params_['min_samples_leaf'] + 4],\r\n",
        "    'min_samples_split': [rf_randomcv.best_params_['min_samples_split'] - 2,\r\n",
        "                          rf_randomcv.best_params_['min_samples_split'] - 1,\r\n",
        "                          rf_randomcv.best_params_['min_samples_split'], \r\n",
        "                          rf_randomcv.best_params_['min_samples_split'] +1,\r\n",
        "                          rf_randomcv.best_params_['min_samples_split'] + 2],\r\n",
        "    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100, \r\n",
        "                     rf_randomcv.best_params_['n_estimators'], \r\n",
        "                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]\r\n",
        "}\r\n",
        "\r\n",
        "print(param_grid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MKMYqVc9dXQ"
      },
      "source": [
        "#### Fit the grid_search to the data\r\n",
        "rf=RandomForestClassifier()\r\n",
        "grid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)\r\n",
        "grid_search.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d2J402s9gkT"
      },
      "source": [
        "grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ0EG5v09h3W"
      },
      "source": [
        "best_grid=grid_search.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQUba-Lq9kM6"
      },
      "source": [
        "y_pred=best_grid.predict(X_test)\r\n",
        "print(confusion_matrix(y_test,y_pred))\r\n",
        "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\r\n",
        "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gceEXTnd9rf7"
      },
      "source": [
        "**Automated Hyperparameter Tuning**\r\n",
        "\r\n",
        "Automated Hyperparameter Tuning can be done by using techniques such as\r\n",
        "\r\n",
        "Bayesian Optimization\r\n",
        "\r\n",
        "Gradient Descent\r\n",
        "\r\n",
        "Evolutionary Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCDwaB8h-ATj"
      },
      "source": [
        "## Bayesian Optimization\r\n",
        "Bayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value.It usually performs better than random,grid and manual search providing better performance in the testing phase and reduced optimization time. In Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin.\r\n",
        "\r\n",
        "Objective Function = defines the loss function to minimize.\r\n",
        "\r\n",
        "Domain Space = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\r\n",
        "\r\n",
        "Optimization Algorithm = defines the search algorithm to use to select the best input values to use in each new iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeS9TNM49xUj"
      },
      "source": [
        "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\r\n",
        "\r\n",
        "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\r\n",
        "        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\r\n",
        "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\r\n",
        "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\r\n",
        "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\r\n",
        "        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\r\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Psc_xHTj-lzA"
      },
      "source": [
        "def objective(space):\r\n",
        "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\r\n",
        "                                 max_features = space['max_features'],\r\n",
        "                                 min_samples_leaf = space['min_samples_leaf'],\r\n",
        "                                 min_samples_split = space['min_samples_split'],\r\n",
        "                                 n_estimators = space['n_estimators'], \r\n",
        "                                 )\r\n",
        "    \r\n",
        "    accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()\r\n",
        "\r\n",
        "    # We aim to maximize accuracy, therefore we return it as a negative value\r\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOx7o-X0-p-C"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\r\n",
        "trials = Trials()\r\n",
        "best = fmin(fn= objective,\r\n",
        "            space= space,\r\n",
        "            algo= tpe.suggest,\r\n",
        "            max_evals = 80,\r\n",
        "            trials= trials)\r\n",
        "best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igZm2ZDD-sdD"
      },
      "source": [
        "crit = {0: 'entropy', 1: 'gini'}\r\n",
        "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\r\n",
        "est = {0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\r\n",
        "\r\n",
        "\r\n",
        "print(crit[best['criterion']])\r\n",
        "print(feat[best['max_features']])\r\n",
        "print(est[best['n_estimators']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igmVKNAN-t7Z"
      },
      "source": [
        "best['min_samples_leaf']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXD4C5F1-wl3"
      },
      "source": [
        "trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], max_depth = best['max_depth'], \r\n",
        "                                       max_features = feat[best['max_features']], \r\n",
        "                                       min_samples_leaf = best['min_samples_leaf'], \r\n",
        "                                       min_samples_split = best['min_samples_split'], \r\n",
        "                                       n_estimators = est[best['n_estimators']]).fit(X_train,y_train)\r\n",
        "predictionforest = trainedforest.predict(X_test)\r\n",
        "print(confusion_matrix(y_test,predictionforest))\r\n",
        "print(accuracy_score(y_test,predictionforest))\r\n",
        "print(classification_report(y_test,predictionforest))\r\n",
        "acc5 = accuracy_score(y_test,predictionforest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnV4qN37-ykZ"
      },
      "source": [
        "## Genetic Algorithms\r\n",
        "Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts.\r\n",
        "\r\n",
        "Let's immagine we create a population of N Machine Learning models with some predifined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again caltulate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_vTirZU-0uj"
      },
      "source": [
        "import numpy as np\r\n",
        "from sklearn.model_selection import RandomizedSearchCV\r\n",
        "# Number of trees in random forest\r\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\r\n",
        "# Number of features to consider at every split\r\n",
        "max_features = ['auto', 'sqrt','log2']\r\n",
        "# Maximum number of levels in tree\r\n",
        "max_depth = [int(x) for x in np.linspace(10, 1000,10)]\r\n",
        "# Minimum number of samples required to split a node\r\n",
        "min_samples_split = [2, 5, 10,14]\r\n",
        "# Minimum number of samples required at each leaf node\r\n",
        "min_samples_leaf = [1, 2, 4,6,8]\r\n",
        "# Create the random grid\r\n",
        "param = {'n_estimators': n_estimators,\r\n",
        "               'max_features': max_features,\r\n",
        "               'max_depth': max_depth,\r\n",
        "               'min_samples_split': min_samples_split,\r\n",
        "               'min_samples_leaf': min_samples_leaf,\r\n",
        "              'criterion':['entropy','gini']}\r\n",
        "print(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UB3csUk-5D6"
      },
      "source": [
        "from tpot import TPOTClassifier\r\n",
        "\r\n",
        "\r\n",
        "tpot_classifier = TPOTClassifier(generations= 5, population_size= 24, offspring_size= 12,\r\n",
        "                                 verbosity= 2, early_stop= 12,\r\n",
        "                                 config_dict={'sklearn.ensemble.RandomForestClassifier': param}, \r\n",
        "                                 cv = 4, scoring = 'accuracy')\r\n",
        "tpot_classifier.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjzYJrrx-_Tm"
      },
      "source": [
        "accuracy = tpot_classifier.score(X_test, y_test)\r\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMxo5ZE3_bcC"
      },
      "source": [
        "## **Optimize hyperparameters of the model using Optuna**\r\n",
        "\r\n",
        "The hyperparameters of the above algorithm are n_estimators and max_depth for which we can try different values to see if the model accuracy can be improved. The objective function is modified to accept a trial object. This trial has several methods for sampling hyperparameters. We create a study to run the hyperparameter optimization and finally read the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo3vdmql_hKs"
      },
      "source": [
        "import optuna\r\n",
        "import sklearn.svm\r\n",
        "def objective(trial):\r\n",
        "\r\n",
        "    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC'])\r\n",
        "    \r\n",
        "    if classifier == 'RandomForest':\r\n",
        "        n_estimators = trial.suggest_int('n_estimators', 200, 2000,10)\r\n",
        "        max_depth = int(trial.suggest_float('max_depth', 10, 100, log=True))\r\n",
        "\r\n",
        "        clf = sklearn.ensemble.RandomForestClassifier(\r\n",
        "            n_estimators=n_estimators, max_depth=max_depth)\r\n",
        "    else:\r\n",
        "        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\r\n",
        "        \r\n",
        "        clf = sklearn.svm.SVC(C=c, gamma='auto')\r\n",
        "\r\n",
        "    return sklearn.model_selection.cross_val_score(\r\n",
        "        clf,X_train,y_train, n_jobs=-1, cv=3).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmX_1nTp_lzC"
      },
      "source": [
        "study = optuna.create_study(direction='maximize')\r\n",
        "study.optimize(objective, n_trials=100)\r\n",
        "\r\n",
        "trial = study.best_trial\r\n",
        "\r\n",
        "print('Accuracy: {}'.format(trial.value))\r\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW4BfXIt_oIa"
      },
      "source": [
        "trial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_3eGKaA_poB"
      },
      "source": [
        "study.best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdlMaXxo_vdq"
      },
      "source": [
        "rf=RandomForestClassifier(n_estimators=330,max_depth=30)\r\n",
        "rf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_ypTcgv_xhF"
      },
      "source": [
        "y_pred=rf.predict(X_test)\r\n",
        "print(confusion_matrix(y_test,y_pred))\r\n",
        "print(accuracy_score(y_test,y_pred))\r\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}