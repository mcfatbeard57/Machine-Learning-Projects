{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "• L2 regularization is 'on' by default (like ridge regression) and controlled by **C** parameter\n",
    "\n",
    "• Larger C means less Regularization\n",
    "\n",
    "• As with regularized linear regression, it can be important to normalize all features so that they are on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression for binary classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,random_state = 0)\n",
    "\n",
    "clf = LogisticRegression(C=100).fit(X_train, y_train)\n",
    "\n",
    "print('Accuracy of Logistic regression classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "print('Accuracy of Logistic regression classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))\n",
    "\n",
    "[clf.predict([[h,w]])[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization shines for data that has higher dimensional feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression regularization: C parameter\n",
    "\n",
    "for this_C, subplot in zip([0.1, 1, 100], subaxes):\n",
    "    clf = LogisticRegression(C=this_C).fit(X_train, y_train)\n",
    "    print('Accuracy of Logistic regression classifier with C = {} on training set: {:.2f}'\n",
    "     .format(this_C, clf.score(X_train, y_train)))\n",
    "    print('Accuracy of Logistic regression classifier with C = {} on test set: {:.2f}\\n'\n",
    "     .format(this_C, clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "**• The strength of regularization is determined by C parameter**\n",
    "\n",
    "**• Larger values of C: less regularization**\n",
    "        \n",
    "        –Fit the training data as well as possible\n",
    "        –Each individual data point is important to classify correctly\n",
    "\n",
    "**• Smaller values of C: more regularization**\n",
    "        \n",
    "        –More tolerant of errors on individual data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n",
    "\n",
    "this_C = 1.0\n",
    "clf = SVC(kernel = 'linear', C=this_C).fit(X_train, y_train)\n",
    "print('Coefficients:\\n', clf.coef_)\n",
    "print('Intercepts:\\n', clf.intercept_)\n",
    "\n",
    "# Linear Support Vector Machine: C parameter\n",
    "for this_C, subplot in zip([0.00001, 100], subaxes):\n",
    "    clf = LinearSVC(C=this_C).fit(X_train, y_train)\n",
    "    print('Accuracy of Linear SVC classifier on training set: {:.2f}'\n",
    "     .format(clf.score(X_train, y_train)))\n",
    "    print('Accuracy of Linear SVC classifier on test set: {:.2f}'\n",
    "     .format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernelized Support Vector Machines\n",
    "\n",
    "Can be used for either Classification or Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "### RBF Kernel\n",
    "\n",
    "• A kernel is a similarity measure (modified dot product) between data points\n",
    "\n",
    "• Transforms input data into higher dimensional feature space and then it is easier to find the hyperplane separting the data.\n",
    "\n",
    "• Still using Maximum Margin principle but due to non-linear transformation, these boundaries are not equally distant from amrgin edges points in orignal input space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "\n",
    "# The default SVC kernel is radial basis function (RBF)\n",
    "clf = SVC().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial kernel, degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'poly', degree = 3).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with RBF kernel: gamma parameter\n",
    "\n",
    "• Smaller Gamma means a large similarity radius i.e. points farther apart are considered similar. More points grouped togetehr Smoother decision Boundary\n",
    "\n",
    "• Larger Gammas means Kernel value deacy quickly point have to be close to be considered similar. More complex tightly restrained decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "\n",
    "for this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n",
    "    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine with RBF kernel: using both C and gamma parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Can also normalized data with feature preprocessing using minmax scaling\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n",
    "\n",
    "for this_gamma, this_axis in zip([0.01, 1, 5], subaxes):   \n",
    "    for this_C, subplot in zip([0.1, 1, 15, 250], this_axis):\n",
    "        title = 'gamma = {:.2f}, C = {:.2f}'.format(this_gamma, this_C)\n",
    "        clf = SVC(kernel = 'rbf', gamma = this_gamma,C = this_C).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
